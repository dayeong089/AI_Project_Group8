{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPX6ky7lwN6x5mw7VA4qsHE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["#Step 0.0: Mount Google Drive in Colab\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I0vbE7TOxsK0","executionInfo":{"status":"ok","timestamp":1700470378731,"user_tz":-540,"elapsed":4095,"user":{"displayName":"Chanhoo Kum","userId":"09646319741024999619"}},"outputId":"92bc5d7b-1d9f-41c3-9b8f-053a3b73abdd"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["#Step 0.1: Install Dependencies\n","\n","!pip install nltk transformers torch\n","import nltk\n","nltk.download('punkt')"],"metadata":{"id":"rTyjqddo6bz6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Step 0.2: Import Libraries\n","\n","import os\n","import json\n","import random\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from nltk.tokenize import sent_tokenize"],"metadata":{"id":"KZ0Xw6bI0cNl","executionInfo":{"status":"ok","timestamp":1700470400030,"user_tz":-540,"elapsed":2,"user":{"displayName":"Chanhoo Kum","userId":"09646319741024999619"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["# Train"],"metadata":{"id":"yJuBkE1cDtYQ"}},{"cell_type":"code","execution_count":26,"metadata":{"id":"Oad1IGNCxKak","executionInfo":{"status":"ok","timestamp":1700470954820,"user_tz":-540,"elapsed":1062,"user":{"displayName":"Chanhoo Kum","userId":"09646319741024999619"}}},"outputs":[],"source":["#Step 1: Read the JSON Files and Preprocess\n","\n","# Function to extract data from a JSON file and derive labels\n","def extract_data_from_json(file_path):\n","    with open(file_path, 'r') as file:\n","        data = json.load(file)\n","        passage = data[\"Meta(Refine)\"][\"passage\"]\n","        summary = data[\"Annotation\"][\"summary3\"]\n","\n","        # Splitting the passage and summary into sentences\n","        passage_sentences = sent_tokenize(passage)\n","        summary_sentences = set(sent_tokenize(summary))\n","\n","        # Label each sentence\n","        labels = [1 if sentence in summary_sentences else 0 for sentence in passage_sentences]\n","\n","        return passage_sentences, labels\n","\n","# Function to prepare data (either training or validation)\n","def prepare_data(file_names, folder_path, num_files):\n","    passages = []\n","    labels_list = []\n","\n","    # Randomly sample 'num_files' from 'file_names'\n","    sampled_files = random.sample(file_names, num_files)\n","\n","    for file_name in sampled_files:\n","        file_path = os.path.join(folder_path, file_name)\n","        passage_sentences, labels = extract_data_from_json(file_path)\n","        passages.extend(passage_sentences)\n","        labels_list.extend(labels)\n","\n","    return passages, labels_list\n","\n","# Read each JSON file, extract data, and derive labels\n","folder_path = '/content/drive/My Drive/20per' #022.요약문 및 레포트 생성 데이터 from AI Hub\n","file_names = os.listdir(folder_path)\n","\n","# Number of files to use for training\n","num_train_files = 5  # Adjust this number as needed\n","\n","# Prepare validation data\n","passages, labels = prepare_data(file_names, folder_path, num_train_files)\n"]},{"cell_type":"code","source":["#Step 2: Create a Dataset Class\n","\n","class TextSummarizationDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len=512):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, item):\n","        text = str(self.texts[item])\n","        label = self.labels[item]\n","\n","        # Tokenize text\n","        encoding = self.tokenizer.encode_plus(\n","          text,\n","          add_special_tokens=True,\n","          max_length=self.max_len,\n","          return_token_type_ids=False,\n","          padding='max_length',\n","          return_attention_mask=True,\n","          return_tensors='pt',\n","          truncation=True\n","        )\n","\n","        return {\n","          'text': text,\n","          'input_ids': encoding['input_ids'].flatten(),\n","          'attention_mask': encoding['attention_mask'].flatten(),\n","          'labels': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","# Load tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","\n","# Create dataset\n","dataset = TextSummarizationDataset(passages, labels, tokenizer)"],"metadata":{"id":"xvGVK-iAxWRl","executionInfo":{"status":"ok","timestamp":1700470962548,"user_tz":-540,"elapsed":1048,"user":{"displayName":"Chanhoo Kum","userId":"09646319741024999619"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["#Step 3: Prepare DataLoader and Training Loop\n","\n","# Data loader\n","data_loader = DataLoader(dataset, batch_size=2)\n","\n","# Load model\n","model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=2)\n","\n","# Training loop\n","optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n","num_epochs = 1\n","total_steps = len(data_loader) * num_epochs\n","\n","print(\"Starting training...\")\n","\n","for epoch in range(num_epochs):\n","    for i, batch in enumerate(data_loader):\n","        input_ids = batch['input_ids']\n","        attention_mask = batch['attention_mask']\n","        labels = batch['labels']\n","\n","        # Forward pass\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","\n","        # Backward pass and optimization\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","        # Calculate current step\n","        current_step = epoch * len(data_loader) + i + 1\n","\n","        # Print progress every 'n' steps (adjust 'n' as needed)\n","        if current_step % 10 == 0:  # Example: Print every 10 steps\n","            percentage_done = (current_step / total_steps) * 100\n","            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{current_step}/{total_steps}]: {percentage_done:.2f}% complete, Loss: {loss.item()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"apSSx1WfxdVE","executionInfo":{"status":"ok","timestamp":1700471424860,"user_tz":-540,"elapsed":459205,"user":{"displayName":"Chanhoo Kum","userId":"09646319741024999619"}},"outputId":"05151881-34d3-4ff9-eb0b-bc4711b2e2d2"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Starting training...\n","Epoch [1/1], Step [10/53]: 18.87% complete, Loss: 0.02224959246814251\n","Epoch [1/1], Step [20/53]: 37.74% complete, Loss: 0.28112056851387024\n","Epoch [1/1], Step [30/53]: 56.60% complete, Loss: 0.26235395669937134\n","Epoch [1/1], Step [40/53]: 75.47% complete, Loss: 0.17496606707572937\n","Epoch [1/1], Step [50/53]: 94.34% complete, Loss: 0.2239627093076706\n"]}]},{"cell_type":"markdown","source":["# Validation"],"metadata":{"id":"Rwfu_q6aDlWh"}},{"cell_type":"code","source":["#Step 1: Validation Data Preparation and DataLoader\n","\n","# File paths for validation data\n","val_folder_path = '/content/drive/My Drive/20per_val' #022.요약문 및 레포트 생성 데이터 from AI Hub\n","val_file_names = os.listdir(val_folder_path)\n","\n","# Number of files to use for validating\n","num_val_files = 3  # Adjust this number as needed\n","\n","# Prepare validation data\n","val_passages, val_labels = prepare_data(val_file_names, val_folder_path, num_val_files)\n","\n","# Validation dataset and data loader\n","val_dataset = TextSummarizationDataset(val_passages, val_labels, tokenizer)\n","val_data_loader = DataLoader(val_dataset, batch_size=2)\n"],"metadata":{"id":"GJmE0IuL2_9G","executionInfo":{"status":"ok","timestamp":1700471831645,"user_tz":-540,"elapsed":2351,"user":{"displayName":"Chanhoo Kum","userId":"09646319741024999619"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["#Step 2: Model Evaluation Function\n","\n","# Function to evaluate the model\n","def evaluate_model(model, data_loader):\n","    model.eval()  # Set model to evaluation mode\n","    predictions = []\n","    actuals = []\n","\n","    with torch.no_grad():\n","        for batch in data_loader:\n","            input_ids = batch['input_ids']\n","            attention_mask = batch['attention_mask']\n","            labels = batch['labels']\n","\n","            outputs = model(input_ids, attention_mask=attention_mask)\n","            logits = outputs.logits\n","            preds = torch.argmax(logits, dim=1).tolist()\n","\n","            predictions.extend(preds)\n","            actuals.extend(labels.tolist())\n","\n","    # Calculate and return metrics here\n","    # Example: Accuracy\n","    accuracy = sum([1 if pred == actual else 0 for pred, actual in zip(predictions, actuals)]) / len(predictions)\n","    return accuracy\n","\n","# Evaluate the model\n","model_accuracy = evaluate_model(model, val_data_loader)\n","print(f\"Validation Accuracy: {model_accuracy}\")"],"metadata":{"id":"4vMDMa1o4v45","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700471909083,"user_tz":-540,"elapsed":73791,"user":{"displayName":"Chanhoo Kum","userId":"09646319741024999619"}},"outputId":"43401caa-8d35-48c0-bc26-5ae25c1402c6"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.8307692307692308\n"]}]}]}